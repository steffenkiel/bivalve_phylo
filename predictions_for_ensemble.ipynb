{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 0\n",
      "Built with CUDA: True\n",
      "Tensorflow version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "# LOAD MODEL WEIGHTS FOR FAMILY, ORDER, AND SUBCLASS CNNS,\n",
    "# AND GET PREDICTIONS FROM EACH FOR THE SAME IMAGE SET\n",
    "\n",
    "from tensorflow.python.keras.applications.resnet_v2 import preprocess_input, ResNet50V2\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.python.keras.models import Sequential, save_model, load_model, optimizers\n",
    "from tensorflow.python.keras.layers import Activation, Dense, Flatten, GlobalAveragePooling2D \n",
    "\n",
    "from tensorflow.python.keras.models import load_model\n",
    "from tensorflow.keras.models import Model\n",
    "#from tensorflow.keras import backend as K\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os, pathlib\n",
    "\n",
    "# load ResNet50_V2\n",
    "resnet = ResNet50V2(include_top=False, pooling=\"avg\", weights='imagenet')\n",
    "\n",
    "print(\"Number of GPUs available:\", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"Built with CUDA:\",tf.test.is_built_with_cuda())\n",
    "print(\"Tensorflow version:\",tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# SET FILE PATHS AND ASSESS IMAGES FOR PREDICTIONS\n",
    "# path to working directory\n",
    "base_path = \"your/working/directory/\"\n",
    "\n",
    "# this directory needs to contain:\n",
    "# 1) the weights of the three models, named \"weights_family.hdf5\", \"weights_order.hdf5\", \"weights_clade.hdf5\"\n",
    "# 2) the class names of the three models, named \"classnames_family.txt\", \"classnames_order.txt\", \"classnames_clade.txt\"\n",
    "# 3) a directory with the prediction images, named \"IMAGES_for_prediction\";\n",
    "#     the images have to be sorted into directories with their respective family names,\n",
    "#     the images have to be named by the name of the family they belong to, and a unique identifier:\n",
    "#     familyname_uniqueIdentifyer.jpg. Example:\n",
    "#     \"your/working/directory/IMAGES_for_prediction/Nuculidae/Nuculidae_idigbio001.jpg\"\n",
    "#     \"your/working/directory/IMAGES_for_prediction/Nuculidae/Nuculidae_idigbio002.jpg\"\n",
    "#     \"your/working/directory/IMAGES_for_prediction/Pectinidae/Pectinidae_gbif001.jpg\"\n",
    "#     \"your/working/directory/IMAGES_for_prediction/Pectinidae/Pectinidae_gbif001.jpg\"\n",
    "\n",
    "#  path to image folder:\n",
    "pred_path = base_path + \"IMAGES_for_prediction/\"\n",
    "image_size = 224\n",
    "\n",
    "img_paths = list(pathlib.Path(pred_path).glob('*/*.jpg'))\n",
    "img_names = [os.path.split(img)[1] for img in img_paths]\n",
    "print(\"Found\",len(img_names),\"images for prediction.\")\n",
    "\n",
    "# get class names of those images\n",
    "img_dirs = pathlib.Path(pred_path)\n",
    "class_names = np.array([item.name for item in img_dirs.glob('*')])\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(f'\\nThey belong to',num_classes,'CLASSES:\\n',class_names)\n",
    "\n",
    "# the predictions-image import function\n",
    "def read_and_prep_images(img_paths, img_height=image_size, img_width=image_size):\n",
    "    imgs = [load_img(img_path, target_size=(img_height, img_width)) for img_path in img_paths]\n",
    "    img_array = np.array([img_to_array(img) for img in imgs])\n",
    "    output = preprocess_input(img_array)\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# taxonomic levels to loop through\n",
    "tax_levels = [\"family\", \"order\", \"clade\"]\n",
    "\n",
    "for taxonomic_level in tax_levels:\n",
    "\n",
    "    #___ get class names\n",
    "    classnames_filename = base_path + \"classnames_\" + taxonomic_level + \".txt\"\n",
    "    model_classnames = open(classnames_filename).read().splitlines()\n",
    "    num_classes = len(model_classnames) \n",
    "    print(f'\\nThe', taxonomic_level, 'model has',num_classes,'classes:\\n',model_classnames)\n",
    "\n",
    "    #___ create model\n",
    "    logits = Dense(num_classes)(resnet.layers[-1].output)\n",
    "    output = Activation('softmax')(logits)\n",
    "    model = Model(resnet.input, output)\n",
    "    model.compile(loss = \"categorical_crossentropy\", metrics=[\"accuracy\"]) \n",
    "\n",
    "    #___ load weights\n",
    "    weights_filename = base_path + \"weights_\" + taxonomic_level + \".hdf5\"\n",
    "    model.load_weights(weights_filename)\n",
    "    print(f'Model created and weights loaded.')\n",
    "\n",
    "    #___ get predictions\n",
    "    # load images for predictions in batches of 1000\n",
    "    test_batches = np.repeat(1000,int(len(img_paths) / 1000))\n",
    "    test_batches = np.append(test_batches,len(img_paths) % 1000)\n",
    "    res_preds = np.zeros(shape=(1,num_classes), dtype=float)\n",
    "\n",
    "    x=0\n",
    "    for i in range(len(test_batches)):\n",
    "        y=x+test_batches[i]\n",
    "\n",
    "        # get images\n",
    "        test_data = read_and_prep_images(img_paths[x:y])\n",
    "\n",
    "        # get predictions    \n",
    "        preds = model.predict(test_data)\n",
    "        res_preds=np.concatenate((res_preds,preds), axis=0)\n",
    "\n",
    "        x=x+test_batches[i]\n",
    "\n",
    "    # remove first row from res_preds; it was just a dummy for initializing...\n",
    "    res_preds = np.delete(res_preds, (0), axis=0)\n",
    "    print(\"Made\", str(len(res_preds)),\"predictions with\", taxonomic_level, \"model.\")\n",
    "\n",
    "    # put predictions into Pandas df and add family names\n",
    "    df=pd.DataFrame(res_preds, columns=model_classnames)\n",
    "    df['image'] = img_names\n",
    "    df['family'] = [x.split('_')[0] for x in img_names]\n",
    "\n",
    "    # save averages by family\n",
    "    families_avg=df.groupby('family').mean()\n",
    "    families_avg.to_csv(base_path +  taxonomic_level + '_predictions_averaged_by_family.csv', index=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
